# Ralph Progress Log
Started: Mon Jan 26 21:15:43 CST 2026

## Codebase Patterns
- WLC Dashboard data is stored in `~/.noc_toolkit/noc_toolkit.db` (not project-local db)
- Passwords are encrypted using Fernet cipher stored in `wlc_dashboard.key`
- Settings save/load functions use INSERT...ON CONFLICT for upsert pattern
- Database migrations use try/except around ALTER TABLE to handle existing columns
- Threading uses daemon threads with wake events for responsive polling
- Credentials are shared between Cisco and Aruba controllers
- Auto-discovery pulls from SolarWinds nodes table
- **IMPORTANT**: `security.py` uses project-local `noc_toolkit.db` for auth, while `db_jobs.py` uses `~/.noc_toolkit/noc_toolkit.db` for data
- Bulk SSH uses ThreadPoolExecutor for parallel execution (default 10 workers)
- Template variables use `{{variable_name}}` syntax, extracted via regex
- Schedule credentials are Fernet-encrypted in database
- **IMPORTANT**: Always use `_CST_TZ = ZoneInfo("America/Chicago")` for timezone-aware datetime in scheduling code
---

## 2026-01-26 21:30 - US-001
- **What was implemented**: Audited WLC Dashboard and Polling System
- **Issues found and fixed**:
  1. **FIXED: Aruba settings not persisted to database** - `aruba_hosts` and `aruba_enabled` were defined in defaults but never saved/loaded from DB
     - Added migration to create `aruba_hosts_json` and `aruba_enabled` columns
     - Updated `load_wlc_dashboard_settings()` to read Aruba settings
     - Updated `save_wlc_dashboard_settings()` to write Aruba settings
  2. **FIXED: Duplicate code block** - Removed duplicate `poll_summary` loading (lines 931-938 had same block twice)
- **Files changed**:
  - `tools/db_jobs.py` - Added Aruba column migrations, fixed load/save functions
- **Verification**:
  - Settings page displays both Cisco 9800 (104) and Aruba 72XX (26) controllers
  - Polling interval displays correctly (5 minutes)
  - Time range buttons work (24h, 3d, 7d, 30d)
  - Stat cards render correctly (Total Clients, Access Points, Polling Status)
  - Aruba settings now persist in database (`aruba_hosts_json`, `aruba_enabled` columns verified)
- **Learnings for future iterations**:
  - Check that new settings fields have corresponding DB columns and are in load/save functions
  - Test settings persistence by restarting app
  - The password hash format must match what `verify_user()` expects (werkzeug pbkdf2)
---

## 2026-01-26 22:15 - US-002
- **What was implemented**: Audited Bulk SSH Tool and Templates
- **Issues found**: None - all functionality working correctly
- **Verification completed**:
  - Template CRUD operations work (create, read, update, delete)
  - Default templates seed correctly (10 common templates)
  - Variable substitution syntax `{{variable}}` displays correctly
  - Template categories (backup, monitoring, routing, troubleshooting) work
  - Job history page renders correctly (empty state shown)
  - Scheduled jobs page renders correctly
  - Main execution page has all required fields:
    - Device selection (paste list, from inventory)
    - Authentication (username, password, enable secret)
    - Command input with template dropdown
    - Options (Show Commands/Config Mode, Device Type, Parallel Connections, Timeout)
  - CSV export route exists and is properly implemented
  - Results storage uses `bulk_ssh_jobs` and `bulk_ssh_results` tables
- **Files changed**: None (no issues found)
- **Code review findings**:
  - BulkSSHJob class in `tools/bulk_ssh.py` uses ThreadPoolExecutor correctly
  - Template engine in `tools/template_engine.py` properly extracts and substitutes variables
  - Database layer in `tools/db_jobs.py` has proper CRUD functions with encryption
  - Schedule worker in `tools/schedule_worker.py` checks every 60 seconds for due jobs
  - Routes in `app.py` lines 5171-5608 handle all Bulk SSH operations
- **Learnings for future iterations**:
  - Bulk SSH templates are stored in `bulk_ssh_templates` table
  - Job results stored in `bulk_ssh_jobs` (metadata) and `bulk_ssh_results` (per-device output)
  - Schedule credentials are encrypted with Fernet before storage
  - Email alerting for schedule failures is a placeholder (not implemented)
---

## 2026-01-27 01:00 - US-003
- **What was implemented**: Audited Bulk SSH Scheduling System
- **Issues found and fixed**:
  1. **FIXED: Timezone mismatch in schedule_worker.py** - `_calculate_next_run()` used naive `datetime.now()` while schedule creation used `datetime.now(_CST_TZ)`. This caused inconsistent timezone handling.
     - Added `ZoneInfo` import and `_CST_TZ = ZoneInfo("America/Chicago")` constant
     - Updated `_calculate_next_run()` to use `datetime.now(_CST_TZ)`
     - Updated `last_run` timestamp to use `datetime.now(_CST_TZ)`
     - Fixed ISE cert sync to handle timezone-aware datetime comparisons
  2. **FIXED: Timezone mismatch in db_jobs.py** - `fetch_due_bulk_ssh_schedules()` used naive `datetime.now()`
     - Added `ZoneInfo` import and `_CST_TZ` constant
     - Updated `fetch_due_bulk_ssh_schedules()` to use `datetime.now(_CST_TZ)`
  3. **ADDED: Missing schedule creation UI** - No UI existed for creating scheduled jobs
     - Added "Create Schedule" button to page header
     - Added modal form with all required fields (name, devices, command, credentials)
     - Schedule type selector (one-time, daily, weekly) with dynamic config sections
     - Alert on failure checkbox and email field
- **Files changed**:
  - `tools/schedule_worker.py` - Timezone fixes, ZoneInfo import
  - `tools/db_jobs.py` - Timezone fix for fetch_due_bulk_ssh_schedules
  - `templates/bulk_ssh_schedules.html` - Added create modal, CSS, JavaScript
- **Verification**:
  - Schedule creation works (tested one-time schedule)
  - Schedule type switching works (one-time, daily, weekly)
  - next_run shows correct timezone offset (-06:00 for CST)
  - Enable/disable toggle works correctly
  - Schedule shows in list with all metadata (type, next_run, last_run, command)
- **Learnings for future iterations**:
  - Always use timezone-aware datetime for scheduling (use `_CST_TZ` constant)
  - Schedule types are: once, daily, weekly (NOT cron expressions)
  - Email alerting is not implemented (placeholder only)
  - Schedule worker checks every 60 seconds via daemon thread
---
