# Ralph Progress Log
Started: Mon Jan 26 21:15:43 CST 2026

## Codebase Patterns
- WLC Dashboard data is stored in `~/.noc_toolkit/noc_toolkit.db` (not project-local db)
- Passwords are encrypted using Fernet cipher stored in `wlc_dashboard.key`
- Settings save/load functions use INSERT...ON CONFLICT for upsert pattern
- Database migrations use try/except around ALTER TABLE to handle existing columns
- Threading uses daemon threads with wake events for responsive polling
- Credentials are shared between Cisco and Aruba controllers
- Auto-discovery pulls from SolarWinds nodes table
- **IMPORTANT**: `security.py` uses project-local `noc_toolkit.db` for auth, while `db_jobs.py` uses `~/.noc_toolkit/noc_toolkit.db` for data
- Bulk SSH uses ThreadPoolExecutor for parallel execution (default 10 workers)
- Template variables use `{{variable_name}}` syntax, extracted via regex
- Schedule credentials are Fernet-encrypted in database
- **IMPORTANT**: Always use `_CST_TZ = ZoneInfo("America/Chicago")` for timezone-aware datetime in scheduling code
- Change statuses: scheduled, running, completed, failed, rollback-running, rolled-back, rollback-failed
- Change scheduler runs every 30 seconds; uses `_CHANGE_WAKE.set()` for immediate wake-up
- SolarWinds API uses SWIS REST endpoint on port 17778 (`/SolarWinds/InformationService/v3/Json/Query`)
- SolarWinds settings use in-memory cache (`_SOLAR_SETTINGS`) with lock, synced to DB on change
- WLC/Aruba hosts auto-detected from SolarWinds nodes via hostname/vendor/model patterns
- **IMPORTANT**: Database functions in `db_jobs.py` use `*,` to force keyword-only args - always call like `func(id, field=value)` not `func(id, {'field': value})`
---

## 2026-01-26 21:30 - US-001
- **What was implemented**: Audited WLC Dashboard and Polling System
- **Issues found and fixed**:
  1. **FIXED: Aruba settings not persisted to database** - `aruba_hosts` and `aruba_enabled` were defined in defaults but never saved/loaded from DB
     - Added migration to create `aruba_hosts_json` and `aruba_enabled` columns
     - Updated `load_wlc_dashboard_settings()` to read Aruba settings
     - Updated `save_wlc_dashboard_settings()` to write Aruba settings
  2. **FIXED: Duplicate code block** - Removed duplicate `poll_summary` loading (lines 931-938 had same block twice)
- **Files changed**:
  - `tools/db_jobs.py` - Added Aruba column migrations, fixed load/save functions
- **Verification**:
  - Settings page displays both Cisco 9800 (104) and Aruba 72XX (26) controllers
  - Polling interval displays correctly (5 minutes)
  - Time range buttons work (24h, 3d, 7d, 30d)
  - Stat cards render correctly (Total Clients, Access Points, Polling Status)
  - Aruba settings now persist in database (`aruba_hosts_json`, `aruba_enabled` columns verified)
- **Learnings for future iterations**:
  - Check that new settings fields have corresponding DB columns and are in load/save functions
  - Test settings persistence by restarting app
  - The password hash format must match what `verify_user()` expects (werkzeug pbkdf2)
---

## 2026-01-26 22:15 - US-002
- **What was implemented**: Audited Bulk SSH Tool and Templates
- **Issues found**: None - all functionality working correctly
- **Verification completed**:
  - Template CRUD operations work (create, read, update, delete)
  - Default templates seed correctly (10 common templates)
  - Variable substitution syntax `{{variable}}` displays correctly
  - Template categories (backup, monitoring, routing, troubleshooting) work
  - Job history page renders correctly (empty state shown)
  - Scheduled jobs page renders correctly
  - Main execution page has all required fields:
    - Device selection (paste list, from inventory)
    - Authentication (username, password, enable secret)
    - Command input with template dropdown
    - Options (Show Commands/Config Mode, Device Type, Parallel Connections, Timeout)
  - CSV export route exists and is properly implemented
  - Results storage uses `bulk_ssh_jobs` and `bulk_ssh_results` tables
- **Files changed**: None (no issues found)
- **Code review findings**:
  - BulkSSHJob class in `tools/bulk_ssh.py` uses ThreadPoolExecutor correctly
  - Template engine in `tools/template_engine.py` properly extracts and substitutes variables
  - Database layer in `tools/db_jobs.py` has proper CRUD functions with encryption
  - Schedule worker in `tools/schedule_worker.py` checks every 60 seconds for due jobs
  - Routes in `app.py` lines 5171-5608 handle all Bulk SSH operations
- **Learnings for future iterations**:
  - Bulk SSH templates are stored in `bulk_ssh_templates` table
  - Job results stored in `bulk_ssh_jobs` (metadata) and `bulk_ssh_results` (per-device output)
  - Schedule credentials are encrypted with Fernet before storage
  - Email alerting for schedule failures is a placeholder (not implemented)
---

## 2026-01-27 01:00 - US-003
- **What was implemented**: Audited Bulk SSH Scheduling System
- **Issues found and fixed**:
  1. **FIXED: Timezone mismatch in schedule_worker.py** - `_calculate_next_run()` used naive `datetime.now()` while schedule creation used `datetime.now(_CST_TZ)`. This caused inconsistent timezone handling.
     - Added `ZoneInfo` import and `_CST_TZ = ZoneInfo("America/Chicago")` constant
     - Updated `_calculate_next_run()` to use `datetime.now(_CST_TZ)`
     - Updated `last_run` timestamp to use `datetime.now(_CST_TZ)`
     - Fixed ISE cert sync to handle timezone-aware datetime comparisons
  2. **FIXED: Timezone mismatch in db_jobs.py** - `fetch_due_bulk_ssh_schedules()` used naive `datetime.now()`
     - Added `ZoneInfo` import and `_CST_TZ` constant
     - Updated `fetch_due_bulk_ssh_schedules()` to use `datetime.now(_CST_TZ)`
  3. **ADDED: Missing schedule creation UI** - No UI existed for creating scheduled jobs
     - Added "Create Schedule" button to page header
     - Added modal form with all required fields (name, devices, command, credentials)
     - Schedule type selector (one-time, daily, weekly) with dynamic config sections
     - Alert on failure checkbox and email field
- **Files changed**:
  - `tools/schedule_worker.py` - Timezone fixes, ZoneInfo import
  - `tools/db_jobs.py` - Timezone fix for fetch_due_bulk_ssh_schedules
  - `templates/bulk_ssh_schedules.html` - Added create modal, CSS, JavaScript
- **Verification**:
  - Schedule creation works (tested one-time schedule)
  - Schedule type switching works (one-time, daily, weekly)
  - next_run shows correct timezone offset (-06:00 for CST)
  - Enable/disable toggle works correctly
  - Schedule shows in list with all metadata (type, next_run, last_run, command)
- **Learnings for future iterations**:
  - Always use timezone-aware datetime for scheduling (use `_CST_TZ` constant)
  - Schedule types are: once, daily, weekly (NOT cron expressions)
  - Email alerting is not implemented (placeholder only)
  - Schedule worker checks every 60 seconds via daemon thread
---

## 2026-01-27 07:00 - US-004
- **What was implemented**: Audited Change Management Workflow
- **Issues found and fixed**:
  1. **FIXED: Missing CSS status classes** - `changes.html` only had styles for `pending`, `completed`, `failed`, `cancelled`, but actual statuses are: `scheduled`, `running`, `rollback-running`, `rolled-back`, `rollback-failed`
     - Added `.status-scheduled` (accent/blue color)
     - Added `.status-running` (info/blue color)
     - Added `.status-rollback-running` (warning/yellow color)
     - Added `.status-rolled-back` (success/green color)
     - Added `.status-rollback-failed` (error/red color)
- **Files changed**:
  - `templates/changes.html` - Added missing CSS status badge classes
- **Verification**:
  - Change window creation with scheduled datetime works
  - Change scheduler loop runs every 30 seconds via daemon thread
  - "Start Now" button triggers immediate execution
  - Status transitions work: scheduled → running → completed/failed
  - Rollback triggers correctly with status: → rollback-running → rolled-back/rollback-failed
  - Timeline shows all change events (created, started, completed, error, rollback-start, rollback-complete)
  - Apply Job Logs and Rollback Job Logs sections display correctly
  - Audit Log link filters correctly by change number
- **Code review findings**:
  - Change scheduler in `app.py` (`_change_scheduler_loop`) runs every 30 seconds
  - Uses `_CHANGE_WAKE` threading.Event for manual wake-up on schedule creation
  - Changes stored in `change_windows` table, events in `change_events` table
  - Credentials encrypted with Fernet before storage via `_encode_change_payload()`
  - Status transitions: scheduled → running → completed/failed, then optionally → rollback-running → rolled-back/rollback-failed
  - Changes created via `/actions/schedule` endpoint (from Interface Search or Global Config tools)
  - Also created via `/tools/wlc/summer-guest/schedule` for WLC Summer Guest automation
- **Learnings for future iterations**:
  - Change statuses: scheduled, running, completed, failed, rollback-running, rolled-back, rollback-failed
  - Scheduler uses UTC for storage, CST for display (via `_format_cst()` helper)
  - Background CLI jobs use `_start_background_cli_job()` which spawns daemon threads
  - Job completion signals `_CHANGE_WAKE.set()` to wake scheduler immediately
---

## 2026-01-27 08:30 - US-005
- **What was implemented**: Audited SolarWinds Integration
- **Issues found**: None - all functionality working correctly
- **Verification completed**:
  - Settings page at `/tools/solarwinds/nodes/settings`:
    - Base URL saves correctly (using SWIS API port 17774)
    - Username persists correctly (supports domain\user format)
    - Password encrypts with Fernet before storage, shown as masked placeholder on reload
    - SSL verification checkbox works correctly
    - "Save Settings" and "Save & Poll" buttons function correctly
  - Connection/poll validation:
    - Auto-port upgrade to SWIS port 17778 works for standard HTTPS
    - API endpoint discovery tries multiple paths (/SolarWinds/InformationService, /Orion/InformationService, etc.)
    - Error messages are descriptive (connection failed, 404, invalid JSON)
  - Node sync verified (17028 nodes fetched):
    - All expected fields populated: node_id, caption, organization, vendor, model, version, ip_address, status, last_seen
    - Organization mapping via CustomProperties.Organization query
    - Vendor auto-detection from MachineType or caption when not provided
    - Extra JSON stored for raw node data
  - Nodes display page at `/tools/solarwinds/nodes`:
    - All fields render correctly in table
    - Client-side filtering works (tested filtering by vendor "cisco" - 9105 matches)
    - Pagination works (100 rows per page)
    - Last poll timestamp and status displayed
    - Links to settings and changes pages
  - Integration points verified:
    - WLC hosts auto-derived from SolarWinds nodes (hostname contains "wc01", vendor "cisco", model "9800")
    - Aruba hosts auto-derived (hostname starts with "wc0", vendor starts with "aruba")
    - Summer guest hosts sync from same WLC detection
    - API endpoint `/api/solarwinds/nodes` returns filtered nodes for bulk SSH inventory search
- **Files changed**: None (no issues found)
- **Code review findings**:
  - SolarWinds API client in `tools/solarwinds.py` with robust endpoint discovery
  - Database functions in `tools/db_jobs.py` lines 1412-1561
  - Settings use in-memory cache with lock (`_SOLAR_SETTINGS_LOCK`) plus DB persistence
  - Poll timestamp uses CST timezone (`datetime.now(_CST_TZ)`)
  - Routes require superadmin for settings, login for viewing nodes
- **Learnings for future iterations**:
  - SolarWinds settings cached in `_SOLAR_SETTINGS` dict with lock protection
  - API uses SWIS (SolarWinds Information Service) REST API on port 17778
  - Organization comes from CustomProperties.Organization in Orion
  - Node sync replaces all nodes (DELETE then INSERT pattern)
  - WLC/Aruba host detection uses specific naming patterns from organization
- Summer Guest scheduler uses `_SUMMER_WAKE` threading.Event for immediate wake-up and waits up to 5 minutes between checks
---

## 2026-01-27 10:15 - US-006
- **What was implemented**: Audited WLC Summer Guest Automation
- **Issues found**: None - all functionality working correctly
- **Verification completed**:
  - Settings page at `/tools/wlc/summer-guest/settings`:
    - Username, password, enable secret save correctly
    - Password encrypted with Fernet before storage
    - Profile names and WLAN IDs persist correctly
    - Poll time (HH:MM format) saves correctly (tested 08:30)
    - Timezone saves correctly (tested America/New_York)
    - Enable scheduler checkbox persists correctly
    - Validation results section displays connection test results
    - "Run Now", "Save & Run", "Save Settings" buttons all work
  - Monitor page at `/tools/wlc/summer-guest`:
    - Status pill shows correct state (Never/OK/Partial/Error)
    - Last poll and next poll timestamps display correctly
    - Next poll shows in configured timezone with proper offset (e.g., "2026-01-28 08:30 AM EST")
    - Targets section shows profile names, WLAN IDs, and auto prefix match
    - Controller filter works for searching host cards
    - Controller cards show hostname, status, and WLAN entries
    - Enable/Disable action popover for immediate toggle
    - Schedule CHG action popover for creating change windows
    - Recent Polls expandable section shows poll history
  - Scheduling integration:
    - Changes created via `/tools/wlc/summer-guest/schedule` endpoint
    - Uses change_windows table for scheduling enable/disable
    - Supports auto-rollback option
    - Change events logged: scheduled, started, completed, error
    - Upcoming changes shown as "Scheduled CHG" badge on controller cards
  - Audit logging verified:
    - WLAN toggle actions logged as `wlc_wlan_toggle` to audit trail
    - WLAN schedule actions logged as `wlc_wlan_schedule` to audit trail
    - Change execution logged via `wlc-summer-toggle` tool in changes.csv
  - Timezone-aware scheduling:
    - `_summer_timezone()` returns ZoneInfo from configurable timezone
    - `_next_summer_run()` calculates next poll in configured timezone
    - `_summer_worker_loop()` uses configured timezone for scheduling
    - Poll timestamps stored and displayed in CST format
- **Files changed**: None (no issues found)
- **Code review findings**:
  - Summer Guest scheduler in `app.py` (`_summer_worker_loop`) uses daemon thread
  - Settings stored in `wlc_summer_settings` table with JSON columns for lists
  - Poll samples stored in `wlc_summer_samples` table with cleanup after 30 days
  - WLC hosts auto-populated from SolarWinds (hostname contains "wc01", vendor "Cisco", model "9800")
  - WLAN state changes via `set_wlan_state()` in `tools/wlc_summer_guest.py`
  - Uses netmiko IOS-XE connection for CLI commands
- **Learnings for future iterations**:
  - Summer Guest has its own configurable timezone (separate from other schedulers)
  - `_SUMMER_WAKE.set()` triggers immediate poll when settings change
  - Scheduler waits up to 5 minutes (300s) between checks when no poll is due
  - Auto prefix match "Summer*" catches all profiles starting with "Summer"
  - WLAN toggle uses: `wlan <profile_name> <wlan_id>`, `no shutdown`/`shutdown`, `write memory`
---

## 2026-01-27 12:00 - US-007
- **What was implemented**: Audited Certificate Tracker and ISE Sync
- **Issues found and fixed**:
  1. **FIXED: `ise_node_add` route passing dict instead of keyword args** - `insert_ise_node(node_data)` was passing a dict, but function expects keyword args
     - Changed to `insert_ise_node(hostname=..., ip=..., username=..., password=..., enabled=True)`
  2. **FIXED: `cert_edit` route passing dict instead of keyword args** - `update_certificate(cert_id, updates)` was passing dict
     - Changed to `update_certificate(cert_id, issued_to=..., issued_by=..., used_by=..., notes=..., devices=...)`
  3. **FIXED: `ise_node_edit` route passing dict instead of keyword args** - `update_ise_node(node_id, updates)` was passing dict
     - Changed to `update_ise_node(node_id, hostname=..., ip=..., username=..., enabled=..., password=...)`
  4. **FIXED: `ise_node_toggle` route passing dict instead of keyword args** - `update_ise_node(node_id, {'enabled': new_status})`
     - Changed to `update_ise_node(node_id, enabled=new_status)`
- **Files changed**:
  - `app.py` - Fixed 4 function calls to use keyword arguments instead of dicts
- **Verification**:
  - Certificate Tracker page loads correctly with stats cards and filter bar
  - ISE Nodes page loads correctly with auto-sync settings
  - Add ISE Node form works - creates node in database
  - Enable/Disable toggle works correctly
  - Delete node works with confirmation dialog
  - All routes pass syntax check
- **Code review findings**:
  - Certificate upload extracts CN, expiration, serial via OpenSSL subprocess calls
  - ISE cert sync uses REST API: `https://{ip}/api/v1/certs/system-certificate/{hostname}`
  - ISE version fetch uses ERS API on port 9060
  - Scheduled sync runs via ScheduleWorker daemon thread every 60 seconds
  - Duplicate detection uses certificate serial number
  - Password encryption uses Fernet cipher
- **Learnings for future iterations**:
  - Database functions use `*,` to force keyword-only arguments - always use `func(id, field=value)` not `func(id, {'field': value})`
  - Certificate expiry status classes: exp-expired, exp-critical, exp-warning, exp-caution, exp-ok, exp-unknown
  - ISE cert API returns: friendlyName, expirationDate, issuedTo, issuedBy, usedBy, serialNumberDecimalFormat
  - Auto-sync settings stored in `cert_sync_settings` table (single row with id=1)
---

## 2026-01-27 14:10 - US-008
- **What was implemented**: Audited Device Inventory Tool
- **Issues found**: None - all functionality working correctly
- **Verification completed**:
  - Device Inventory page (`/tools/device-inventory`) displays SolarWinds data correctly
  - Stats cards showing: Total 17028, Cisco 9105, Dell 3149, Unknown 2664, Net-SNMP 728, Tripp Lite 339
  - Filter bar works: search, vendor dropdown, organization dropdown, model/version text filters
  - "Clear" button appears when filters are active
  - Device table renders correctly with columns: Hostname, IP, Organization, Vendor, Model, Version, Status
  - Vendor color badges work (Cisco blue, Aruba orange, Dell blue)
  - Status indicators work (Up green, Down red)
  - CSV export downloads correctly with filtered data, filename format: `device_inventory_YYYYMMDD_HHMMSS.csv`
  - Device scanning code supports: Cisco IOS, IOS-XE, NX-OS, Aruba AOS, Dell OS10/Force10/PowerConnect
  - Database CRUD operations verified (insert/update/get/delete)
  - Delete endpoint requires superadmin role (correctly implemented)
  - SolarWinds node linking via `solarwinds_node_id` field in schema
- **Files changed**: None (no issues found)
- **Code review findings**:
  - `tools/device_inventory.py` - Device scanning with ThreadPoolExecutor (max 50 workers)
  - `tools/db_jobs.py` - Device inventory CRUD with proper ON CONFLICT upsert
  - `tools/netmiko_helpers.py` - Connection context managers for IOS, IOS-XE, Aruba AOS
  - Routes in `app.py` lines 4632-4861 handle display, scan, export, delete, API
  - Page primarily displays SolarWinds data; scan/delete are for locally-scanned devices
- **Learnings for future iterations**:
  - Device Inventory has dual data sources: SolarWinds (display) and local scans (enrichment)
  - Local scan results stored in `device_inventory` table, SolarWinds data in `solarwinds_nodes` table
  - Export uses SolarWinds data with current filters applied
  - Scan endpoint accepts hosts (comma/newline), credentials, device_type, max_workers
  - No UI for manual scanning currently - would need to add if required
---

## 2026-01-27 15:30 - US-009
- **What was implemented**: Audited Topology Builder
- **Issues found**: None - all functionality working correctly
- **Verification completed**:
  - Topology Builder page (`/tools/topology`) renders correctly with three tabs:
    - Single Device: Device selector with SolarWinds datalist, credentials, device type dropdown
    - Organization: Organization selector with SolarWinds orgs, credentials, device type
    - Bulk Discovery: Device list textarea, credentials, device type (Cisco/Dell only)
  - Graph View page (`/tools/topology/graph`) renders correctly:
    - Cytoscape.js with dagre layout for hierarchical top-down visualization
    - Graph controls: Fit, Reset Zoom, Layout Refresh, Center Selected
    - Stats bar showing node/edge counts
    - Legend showing Root Device (purple), Network Device (green), Expanded Node (gray)
    - View toggle between Graph View and Table View
    - Node selection panel with details and "Discover Neighbors" button
  - CDP/LLDP neighbor collection:
    - `parse_cdp_detail()` parses "show cdp neighbors detail" output
    - `parse_lldp_detail()` parses "show lldp neighbors detail" output
    - `_merge_neighbors()` combines CDP and LLDP findings intelligently
    - `annotate_with_inventory()` enriches neighbors with SolarWinds metadata
  - Multi-vendor support verified:
    - Cisco IOS/XE: cisco_ios, cisco_xe device types
    - Dell: dell_os10, dell_force10, dell_powerconnect device types
    - Auto-detection tries Cisco first, then Dell variants
  - Device type auto-detection:
    - `_build_device_type_candidates()` builds ordered list of device types to try
    - Uses vendor hint from SolarWinds or manual mode selection
    - Graceful fallback on connection failures
  - CSV export functionality verified via `/tools/topology/export`
  - API endpoint `/api/topology/discover` for dynamic graph expansion
- **Files changed**: None (no issues found)
- **Code review findings**:
  - `tools/topology.py` - Core CDP/LLDP parsing and neighbor merging logic
  - Routes in `app.py` lines 2442-2841 handle all topology operations
  - Cytoscape.js 3.26.0 with dagre 2.5.0 extension for graph visualization
  - Hierarchy detection based on device naming (cs/core, mor, tor, ds/dist, ag/agg, as/access)
  - Access Points (AP*) are filtered out from graph visualization
  - No persistent database storage - topology data is transient, calculated on-demand
- **Learnings for future iterations**:
  - Topology data is ephemeral - discovered on-demand, not stored in database
  - CDP/LLDP parsing uses regex patterns for flexibility across vendor outputs
  - Graph expansion uses POST to `/api/topology/discover` with stored credentials
  - AP filtering is implemented both in backend and frontend (hideAPToggle checkbox)
  - SolarWinds inventory matching uses IP address first, then caption (hostname)
---

## 2026-01-27 17:00 - US-010
- **What was implemented**: Audited Authentication and Admin Panel
- **Issues found**: None - all functionality working correctly
- **Verification completed**:
  - Login page renders correctly with username/password form and theme toggle
  - Login with admin/admin123 works, sets session (user_id, username, role)
  - Welcome flash message displays on successful login
  - Login failures are logged via `log_audit()` with action "login_failed"
  - Logout clears session, logs action, displays flash message
  - Admin Users panel (`/admin/users`):
    - Create New User form with: username, password (min 8 chars), role dropdown (user/superadmin), KB access level (FSR/NOC/Admin), can_create_kb checkbox
    - User table displays: ID, username, role badge, KB permissions with inline edit, created date, last login
    - KB permissions inline form allows updating access level and create permission
    - Superadmins have disabled create checkbox (always have permission)
    - User creation correctly inserts to `users` table with password_hash (pbkdf2:sha256)
  - Page Settings panel (`/admin/page-settings`):
    - All pages grouped by category (Main, Config Tools, WLC Tools, Infrastructure, Certificates)
    - Toggle switches with status dots (green=enabled, red=disabled)
    - Enable All / Disable All buttons work
    - Save Changes updates `page_settings` table via `bulk_update_page_settings()`
    - `@require_page_enabled` decorator blocks access to disabled pages
  - Profile page (`/profile`):
    - Shows username and role (read-only)
    - Password change form validates: current password, new password >= 8 chars, confirmation match
    - Password change logged via `log_audit()`
  - Knowledge Base (`/knowledge-base`):
    - "New Article" button only visible to users with `can_create_kb` or superadmin role
    - Articles filtered by user's `kb_access_level` (FSR < NOC < Admin hierarchy)
    - Visibility badges display correctly (FSR green, NOC blue, Admin orange)
    - Search and subject filter work correctly
  - Audit Logging:
    - Auth actions logged to `audit_log` SQLite table via `log_audit()` in security.py
    - Config changes logged to `logs/changes.csv` via `_log_audit()` in app.py
    - Audit Log Viewer at `/logs` shows CSV data with filtering, search, pagination, CSV export
    - 15+ log entries displaying correctly with timestamps, users, tools, results
- **Files changed**: None (no issues found)
- **Code review findings**:
  - `tools/security.py` - Core auth functions: encrypt/decrypt password, create_user, verify_user, change_password, log_audit, require_login/require_superadmin/require_kb_create/require_page_enabled decorators
  - `tools/db_jobs.py` lines 2812-2916 - Page settings CRUD functions
  - `app.py` lines 1729-1890 - Login/logout/profile/admin routes
  - `app.py` lines 6360-6569 - KB article functions and routes
  - Password hashing uses werkzeug pbkdf2:sha256
  - Session stores user_id, username, role
  - KB access levels: FSR (1) < NOC (2) < Admin (3)
- **Learnings for future iterations**:
  - Two audit systems: SQLite `audit_log` table (auth) and `logs/changes.csv` (config changes)
  - Page visibility uses `page_settings` table with `@require_page_enabled` decorator
  - KB access is hierarchical: Admin sees all, NOC sees NOC+FSR, FSR sees only FSR
  - Superadmins always have KB create permission (checked in `can_user_create_kb()`)
  - User passwords use werkzeug's pbkdf2:sha256 method, device passwords use Fernet encryption
---
